{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a0000001",
            "metadata": {},
            "source": [
                "# üß† AI Twin ‚Äî Data Preprocessing & Hugging Face Push Pipeline\n",
                "\n",
                "This notebook takes scraped text data (website, LinkedIn, CV, SOP, chat logs),\n",
                "cleans and chunks it, extracts structured information using OpenAI,\n",
                "converts everything into instruction-tuning JSONL format, and pushes\n",
                "the final dataset to a **private** Hugging Face repository.\n",
                "\n",
                "### Notebook Structure\n",
                "| Step | Description |\n",
                "|------|-------------|\n",
                "| 0 | Load environment variables & libraries |\n",
                "| 1 | Load scraped data |\n",
                "| 2 | Clean & preprocess text |\n",
                "| 3 | Chunk text into ~3 000-char blocks |\n",
                "| 4 | Extract structured info via OpenAI |\n",
                "| 5 | Save locally as `.jsonl` |\n",
                "| 6 | Push dataset to Hugging Face Hub |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000002",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 0 ‚Äî Load Environment Variables & Libraries\n",
                "\n",
                "We read API keys from the `.env` file and import every library we'll need.\n",
                "Make sure your `.env` contains:\n",
                "```\n",
                "OPENAI_API_KEY = sk-...\n",
                "HF_TOKEN = hf_...\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "a0000003",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Environment loaded and authenticated.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 0: Environment & Imports\n",
                "# ============================================================\n",
                "\n",
                "import os\n",
                "import re\n",
                "import json\n",
                "import time\n",
                "import textwrap\n",
                "from pathlib import Path\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from openai import OpenAI\n",
                "from tqdm.notebook import tqdm          # progress bars inside Jupyter\n",
                "from datasets import Dataset\n",
                "from huggingface_hub import login as hf_login\n",
                "\n",
                "# Load .env file\n",
                "load_dotenv()\n",
                "\n",
                "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
                "HF_TOKEN       = os.getenv(\"HF_TOKEN\")\n",
                "\n",
                "assert OPENAI_API_KEY, \"‚ùå OPENAI_API_KEY not found in .env\"\n",
                "assert HF_TOKEN,       \"‚ùå HF_TOKEN not found in .env\"\n",
                "\n",
                "# Initialize OpenAI client\n",
                "client = OpenAI(api_key=OPENAI_API_KEY)\n",
                "MODEL  = \"gpt-4o-mini\"   # fast & cheap; swap to gpt-4o if you want higher quality\n",
                "\n",
                "# Authenticate with Hugging Face\n",
                "hf_login(token=HF_TOKEN)\n",
                "\n",
                "print(\"‚úÖ Environment loaded and authenticated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "204e48a9",
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install pymupdf"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000004",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1 ‚Äî Load Scraped Data\n",
                "\n",
                "All raw text from your various sources goes into the `scraped_data` dictionary.\n",
                "Replace the placeholder strings below with your **actual scraped content**,\n",
                "or load them from files.\n",
                "\n",
                "> **Tip:** If you already ran `web_data(scrap).ipynb`, you can copy-paste\n",
                "> the `scraped_data` dict from that notebook, or load the saved markdown file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "a0000005",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÑ Personal Knowledge Base length: 5,141 chars\n",
                        "üìÑ CV length:  6,033 chars\n",
                        "üìÑ SOP length: 3,848 chars\n",
                        "üìÑ PS length:  3,129 chars\n",
                        "üìÑ Chat logs length: 7,346,040 chars\n",
                        "\n",
                        " Data source sizes:\n",
                        "   Personal_Knowledge_Base             ‚Üí      5,141 chars\n",
                        "   CV                                  ‚Üí      6,033 chars\n",
                        "   SOP                                 ‚Üí      3,848 chars\n",
                        "   PersonalStatement                   ‚Üí      3,129 chars\n",
                        "   ChatLogs                            ‚Üí  7,346,040 chars\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 1: Load Scraped Data (Markdown + PDFs)\n",
                "# ============================================================\n",
                "\n",
                "# --- Imports ---\n",
                "from pathlib import Path\n",
                "import fitz  # PyMuPDF\n",
                "\n",
                "# --- 1a. Load the Personal Knowledge Base markdown ---\n",
                "knowledge_base_path = Path(r\"G:\\Github_Projects\\Ai_twin\\file\\CV_statement_details\\Personal_Knowledge_Base.md\")\n",
                "knowledge_base_text = knowledge_base_path.read_text(encoding=\"utf-8\") if knowledge_base_path.exists() else \"\"\n",
                "print(f\"üìÑ Personal Knowledge Base length: {len(knowledge_base_text):,} chars\")\n",
                "\n",
                "# --- 1b. Load CV / SOP / Personal Statement PDFs ---\n",
                "def read_pdf(path: str) -> str:\n",
                "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
                "    doc = fitz.open(path)\n",
                "    pages = [page.get_text() for page in doc]\n",
                "    doc.close()\n",
                "    return \"\\n\".join(pages)\n",
                "\n",
                "cv_text       = read_pdf(r\"G:\\Github_Projects\\Ai_twin\\file\\CV_statement_details\\Md_Maruf_Mullah_CV.pdf\")\n",
                "sop_text      = read_pdf(r\"G:\\Github_Projects\\Ai_twin\\file\\CV_statement_details\\Statement of Purpose of Md. Maruf Mullah.pdf\")\n",
                "ps_text       = read_pdf(r\"G:\\Github_Projects\\Ai_twin\\file\\CV_statement_details\\Personal Statement of Md. Maruf Mullah.pdf\")\n",
                "chat_logs_text = read_pdf(r\"G:\\Github_Projects\\Ai_twin\\file\\chat\\chat50.pdf\")\n",
                "\n",
                "print(f\"üìÑ CV length:  {len(cv_text):,} chars\")\n",
                "print(f\"üìÑ SOP length: {len(sop_text):,} chars\")\n",
                "print(f\"üìÑ PS length:  {len(ps_text):,} chars\")\n",
                "print(f\"üìÑ Chat logs length: {len(chat_logs_text):,} chars\")\n",
                "\n",
                "# --- 1c. Assemble the master dictionary including Markdown ---\n",
                "scraped_data = {\n",
                "    \"Personal_Knowledge_Base\": knowledge_base_text,\n",
                "    \"CV\": cv_text,\n",
                "    \"SOP\": sop_text,\n",
                "    \"PersonalStatement\": ps_text,\n",
                "    \"ChatLogs\": chat_logs_text,\n",
                "}\n",
                "\n",
                "# Quick overview\n",
                "print(\"\\n Data source sizes:\")\n",
                "for label, text in scraped_data.items():\n",
                "    print(f\"   {label:35s} ‚Üí {len(text):>10,} chars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000006",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2 ‚Äî Clean & Preprocess Text\n",
                "\n",
                "We apply several cleaning rules:\n",
                "1. Strip HTML artifacts, URLs, and excessive whitespace\n",
                "2. Remove navigation / footer / cookie-banner boilerplate\n",
                "3. Drop tiny junk lines (< 20 chars)\n",
                "4. Collapse multiple blank lines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "a0000007",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "77f6dca651e147c78ede04545be9f496",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Cleaning:   0%|          | 0/5 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Personal_Knowledge_Base: 5,141 -> 4,653 chars  (90.5% kept)\n",
                        "  CV: 6,033 -> 5,658 chars  (93.8% kept)\n",
                        "  SOP: 3,848 -> 3,676 chars  (95.5% kept)\n",
                        "  PersonalStatement: 3,129 -> 3,039 chars  (97.1% kept)\n",
                        "  ChatLogs: 7,346,040 -> 5,183,960 (cleaned) -> 19,995 chars  (capped to 20,000)\n",
                        "\n",
                        "Final sizes after cleaning + capping:\n",
                        "   Personal_Knowledge_Base             ->      4,653 chars\n",
                        "   CV                                  ->      5,658 chars\n",
                        "   SOP                                 ->      3,676 chars\n",
                        "   PersonalStatement                   ->      3,039 chars\n",
                        "   ChatLogs                            ->     19,995 chars\n",
                        "   TOTAL                               ->     37,021 chars\n",
                        "\n",
                        "Estimated chunks: ~12\n",
                        "\n",
                        "‚úÖ Cleaning complete.\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 2: Clean & Preprocess Text\n",
                "# ============================================================\n",
                "\n",
                "# Words that signal navigation / boilerplate noise\n",
                "NOISE_KEYWORDS = [\n",
                "    \"menu\", \"navigation\", \"navbar\", \"footer\", \"sidebar\",\n",
                "    \"cookie\", \"privacy policy\", \"terms of service\",\n",
                "    \"sign in\", \"sign up\", \"log in\", \"log out\",\n",
                "    \"subscribe\", \"newsletter\", \"advertisement\",\n",
                "    \"all rights reserved\", \"copyright\",\n",
                "]\n",
                "\n",
                "MIN_LINE_LENGTH = 20  # discard lines shorter than this\n",
                "\n",
                "# ---- Cost-control: cap large sources ----\n",
                "# ChatLogs alone is ~5 M chars ‚Üí ~2000+ chunks ‚Üí $$$\n",
                "# We keep only the first N chars (at a clean boundary).\n",
                "MAX_CHARS = {\n",
                "    \"ChatLogs\": 20_000,   # <-- adjust this number as needed\n",
                "}\n",
                "\n",
                "\n",
                "def clean_text(text: str) -> str:\n",
                "    \"\"\"Deep-clean a block of text.\"\"\"\n",
                "    # 1. Remove leftover HTML tags\n",
                "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
                "\n",
                "    # 2. Remove URLs\n",
                "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
                "\n",
                "    # 3. Normalize whitespace within lines\n",
                "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
                "\n",
                "    # 4. Split into lines, strip, filter\n",
                "    lines = [line.strip() for line in text.splitlines()]\n",
                "    cleaned_lines = []\n",
                "    for line in lines:\n",
                "        if len(line) < MIN_LINE_LENGTH:\n",
                "            continue\n",
                "        low = line.lower()\n",
                "        if any(kw in low for kw in NOISE_KEYWORDS):\n",
                "            continue\n",
                "        cleaned_lines.append(line)\n",
                "\n",
                "    # 5. Collapse multiple blank lines\n",
                "    result = \"\\n\".join(cleaned_lines)\n",
                "    result = re.sub(r\"\\n{3,}\", \"\\n\\n\", result)\n",
                "    return result.strip()\n",
                "\n",
                "\n",
                "def smart_truncate(text: str, max_chars: int) -> str:\n",
                "    \"\"\"Truncate text at a clean paragraph or sentence boundary.\"\"\"\n",
                "    if len(text) <= max_chars:\n",
                "        return text\n",
                "    # Try to cut at a paragraph break\n",
                "    cut = text.rfind('\\n\\n', 0, max_chars)\n",
                "    if cut == -1:\n",
                "        # Fall back to sentence break\n",
                "        cut = text.rfind('. ', 0, max_chars)\n",
                "    if cut == -1:\n",
                "        cut = max_chars\n",
                "    return text[:cut].strip()\n",
                "\n",
                "\n",
                "# Apply cleaning to every source\n",
                "cleaned_data = {}\n",
                "for label, raw_text in tqdm(scraped_data.items(), desc=\"Cleaning\"):\n",
                "    cleaned = clean_text(raw_text)\n",
                "\n",
                "    # Apply per-source character cap (if configured)\n",
                "    if label in MAX_CHARS:\n",
                "        before = len(cleaned)\n",
                "        cleaned = smart_truncate(cleaned, MAX_CHARS[label])\n",
                "        print(f\"  {label}: {len(raw_text):,} -> {before:,} (cleaned) -> {len(cleaned):,} chars  (capped to {MAX_CHARS[label]:,})\")\n",
                "    else:\n",
                "        print(f\"  {label}: {len(raw_text):,} -> {len(cleaned):,} chars  \"\n",
                "              f\"({100 * len(cleaned) / max(len(raw_text), 1):.1f}% kept)\")\n",
                "\n",
                "    cleaned_data[label] = cleaned\n",
                "\n",
                "print(f\"\\nFinal sizes after cleaning + capping:\")\n",
                "for label, text in cleaned_data.items():\n",
                "    print(f\"   {label:35s} -> {len(text):>10,} chars\")\n",
                "total = sum(len(t) for t in cleaned_data.values())\n",
                "print(f\"   {'TOTAL':35s} -> {total:>10,} chars\")\n",
                "print(f\"\\nEstimated chunks: ~{total // 3000}\")\n",
                "print(\"\\n‚úÖ Cleaning complete.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000008",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3 ‚Äî Chunk Text into ~3 000-char Blocks\n",
                "\n",
                "Large texts need to be split into manageable chunks before we send them to\n",
                "the LLM for extraction. We use a sliding-window approach that tries to\n",
                "break at paragraph boundaries, with a small overlap to preserve context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "a0000009",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "634328773ef44f68bf7d8e4ebedd3279",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Chunking:   0%|          | 0/5 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Personal_Knowledge_Base: 2 chunks\n",
                        "  CV: 3 chunks\n",
                        "  SOP: 2 chunks\n",
                        "  PersonalStatement: 2 chunks\n",
                        "  ChatLogs: 9 chunks\n",
                        "\n",
                        "‚úÖ Total chunks: 18\n",
                        "   Estimated memory: 39 KB\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 3: Chunk Text  (memory-safe, infinite-loop-proof)\n",
                "# ============================================================\n",
                "\n",
                "import gc\n",
                "\n",
                "CHUNK_SIZE    = 3000   # target characters per chunk\n",
                "CHUNK_OVERLAP = 200    # overlap between consecutive chunks\n",
                "\n",
                "\n",
                "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE,\n",
                "               overlap: int = CHUNK_OVERLAP):\n",
                "    \"\"\"\n",
                "    Generator that yields chunks of ~`chunk_size` characters.\n",
                "    Tries to break at paragraph or sentence boundaries.\n",
                "\n",
                "    Key fix:  `start` is GUARANTEED to advance by at least\n",
                "    `min_advance` characters on every iteration, so the loop\n",
                "    always terminates ‚Äî even on multi-million-character texts.\n",
                "    \"\"\"\n",
                "    if not text.strip():\n",
                "        return\n",
                "\n",
                "    length      = len(text)\n",
                "    min_advance = max(chunk_size // 2, 1)   # never advance less than half a chunk\n",
                "    start       = 0\n",
                "\n",
                "    while start < length:\n",
                "        end = min(start + chunk_size, length)\n",
                "\n",
                "        if end < length:\n",
                "            # Try to break at a paragraph boundary\n",
                "            bp = text.rfind('\\n\\n', start + min_advance, end)\n",
                "            if bp == -1:\n",
                "                # Fall back to sentence-ending period\n",
                "                bp = text.rfind('. ', start + min_advance, end)\n",
                "            if bp == -1:\n",
                "                # Fall back to any newline\n",
                "                bp = text.rfind('\\n', start + min_advance, end)\n",
                "            if bp != -1:\n",
                "                end = bp + 1\n",
                "\n",
                "        chunk = text[start:end].strip()\n",
                "        if chunk:\n",
                "            yield chunk\n",
                "\n",
                "        # Guarantee forward progress\n",
                "        next_start = end - overlap if end < length else length\n",
                "        if next_start <= start:\n",
                "            next_start = start + min_advance   # safety: force advance\n",
                "        start = next_start\n",
                "\n",
                "\n",
                "# --- Chunk every cleaned source, one at a time ---\n",
                "all_chunks = []\n",
                "\n",
                "for label, text in tqdm(cleaned_data.items(), desc='Chunking'):\n",
                "    chunk_count = 0\n",
                "    for i, chunk in enumerate(chunk_text(text)):\n",
                "        all_chunks.append({\n",
                "            'source':      label,\n",
                "            'chunk_index': i,\n",
                "            'text':        chunk,\n",
                "        })\n",
                "        chunk_count += 1\n",
                "    print(f'  {label}: {chunk_count} chunks')\n",
                "    gc.collect()           # free intermediate objects between large sources\n",
                "\n",
                "print(f'\\n‚úÖ Total chunks: {len(all_chunks)}')\n",
                "print(f'   Estimated memory: {sum(len(c[\"text\"]) for c in all_chunks) / 1024:.0f} KB')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000010",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4 ‚Äî Extract Structured Info Using OpenAI\n",
                "\n",
                "For each chunk we ask the LLM to:\n",
                "1. **Summarize** the content\n",
                "2. **Extract** structured facts (education, skills, projects, etc.)\n",
                "3. Return the result as a **JSON object**\n",
                "\n",
                "We then convert each response into an **instruction-tuning** sample:\n",
                "```json\n",
                "{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}\n",
                "```\n",
                "\n",
                "> ‚è± This step makes API calls ‚Äî expect ~1-2 seconds per chunk.\n",
                "> A progress bar shows real-time status."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a0000011",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7e448df91cb541e68c3e4af6637ac8d6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Extracting structured info:   0%|          | 0/18 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ Extraction complete.\n",
                        "   Processed: 18\n",
                        "   Failed:    0\n",
                        "   Saved to:  extracted_records.jsonl\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 4: Extraction (Optimized & Resume-Safe)\n",
                "# ============================================================\n",
                "\n",
                "import textwrap\n",
                "\n",
                "# üî• Define the prompt that was missing\n",
                "SYSTEM_PROMPT = textwrap.dedent(\"\"\"\\\n",
                "    You are an expert AI assistant building a structured knowledge base\n",
                "    about a person named Md. Maruf Mullah. Your job is to extract factual,\n",
                "    biographical, academic, professional, and personality-related information\n",
                "    from a chunk of text so it can be used for an AI Twin and RAG system.\n",
                "\n",
                "    Return your answer as a **valid JSON object** with these keys:\n",
                "    {\n",
                "      \"summary\": \"<2-3 sentence summary of the chunk>\",\n",
                "      \"category\": \"<one of: biography, education, skills, projects, research,\n",
                "                    experience, achievements, goals, values, writing_style,\n",
                "                    contact, chat, other>\",\n",
                "      \"key_facts\": [\"<fact 1>\", \"<fact 2>\", ...],\n",
                "      \"instruction_prompt\": \"<a natural question a user might ask that\n",
                "                              this chunk answers>\",\n",
                "      \"ideal_response\": \"<a complete, conversational answer to that question\n",
                "                          using ONLY information in this chunk>\"\n",
                "    }\n",
                "\n",
                "    Rules:\n",
                "    - Only use facts present in the text\n",
                "    - Do NOT invent anything\n",
                "    - Keep the ideal_response in first person as Maruf would say it\n",
                "    - Return ONLY the JSON, NO markdown fences\n",
                "\"\"\")\n",
                "\n",
                "OUTPUT_FILE = \"extracted_records.jsonl\"\n",
                "\n",
                "def extract_info(chunk_text: str, source_label: str,\n",
                "                 max_retries: int = 3):\n",
                "\n",
                "    user_prompt = f\"Source: {source_label}\\n\\nCONTENT:\\n{chunk_text}\"\n",
                "\n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=MODEL,\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "                    {\"role\": \"user\",   \"content\": user_prompt},\n",
                "                ],\n",
                "                temperature=0.2,      # lower = more deterministic JSON\n",
                "                max_tokens=700,       # üî• reduce from 1024 (saves cost)\n",
                "            )\n",
                "\n",
                "            raw = response.choices[0].message.content.strip()\n",
                "\n",
                "            # Remove markdown fences safely\n",
                "            raw = re.sub(r\"^```(?:json)?\\s*\", \"\", raw)\n",
                "            raw = re.sub(r\"\\s*```$\", \"\", raw)\n",
                "\n",
                "            return json.loads(raw)\n",
                "\n",
                "        except json.JSONDecodeError:\n",
                "            print(\"‚ö†Ô∏è JSON decode failed. Retrying...\")\n",
                "            time.sleep(2 ** attempt)\n",
                "\n",
                "        except Exception as e:\n",
                "            if attempt < max_retries - 1:\n",
                "                time.sleep(2 ** attempt)\n",
                "            else:\n",
                "                print(f\"‚ö†Ô∏è Failed: {e}\")\n",
                "                return None\n",
                "\n",
                "\n",
                "# ============================================================\n",
                "# Run Extraction (Memory Safe + Resume Safe)\n",
                "# ============================================================\n",
                "\n",
                "processed = 0\n",
                "failed = 0\n",
                "\n",
                "# Open file in append mode (safe if kernel stops)\n",
                "with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
                "\n",
                "    for chunk in tqdm(all_chunks, desc=\"Extracting structured info\"):\n",
                "\n",
                "        result = extract_info(chunk[\"text\"], chunk[\"source\"])\n",
                "\n",
                "        if result:\n",
                "            result[\"source\"] = chunk[\"source\"]\n",
                "            result[\"chunk_index\"] = chunk[\"chunk_index\"]\n",
                "\n",
                "            f.write(json.dumps(result) + \"\\n\")  # üî• write immediately\n",
                "            processed += 1\n",
                "        else:\n",
                "            failed += 1\n",
                "        \n",
                "        # Small delay to respect rate limits\n",
                "        time.sleep(0.5)\n",
                "\n",
                "print(f\"\\n‚úÖ Extraction complete.\")\n",
                "print(f\"   Processed: {processed}\")\n",
                "print(f\"   Failed:    {failed}\")\n",
                "print(f\"   Saved to:  {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000012",
            "metadata": {},
            "source": [
                "### 4b ‚Äî Convert to Instruction-Tuning JSONL Format\n",
                "\n",
                "Each record is transformed into the standard `instruction / input / output`\n",
                "format used for supervised fine-tuning of LLMs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "a0000013",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Loaded 49 records from extracted_records.jsonl\n",
                        "‚úÖ Total instruction-tuning samples: 147\n",
                        "\n",
                        "--- Sample Preview ---\n",
                        "{\n",
                        "  \"instruction\": \"What are Md. Maruf Mullah's professional background and research interests?\",\n",
                        "  \"input\": \"\",\n",
                        "  \"output\": \"I am a Mechanical Engineer and Researcher with a strong focus on bridging classical engineering and computational intelligence. My research interests include machine learning, materials science, robotics, and renewable energy applications. I have industrial experience at IFAD Autos PLC and PRAN-RFL Group, and I achieved 99.9% accuracy in a casting defect classification project.\",\n",
                        "  \"source\": \"Personal_Knowledge_Base\",\n",
                        "  \"category\": \"biography\"\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 4b: Build instruction-tuning samples\n",
                "# ============================================================\n",
                "\n",
                "instruction_samples = []\n",
                "OUTPUT_FILE = \"extracted_records.jsonl\"\n",
                "\n",
                "# üî• Load records from the file we just saved\n",
                "try:\n",
                "    with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
                "        extracted_records = [json.loads(line) for line in f]\n",
                "    print(f\"‚úÖ Loaded {len(extracted_records)} records from {OUTPUT_FILE}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"‚ö†Ô∏è Error: {OUTPUT_FILE} not found. Did Step 4 run?\")\n",
                "    extracted_records = []\n",
                "\n",
                "for rec in extracted_records:\n",
                "    # Primary sample: question ‚Üí answer\n",
                "    instruction_samples.append({\n",
                "        \"instruction\": rec.get(\"instruction_prompt\", \"Tell me about yourself.\"),\n",
                "        \"input\":       \"\",  # no extra input needed\n",
                "        \"output\":      rec.get(\"ideal_response\", rec.get(\"summary\", \"\")),\n",
                "        \"source\":      rec.get(\"source\", \"unknown\"),\n",
                "        \"category\":    rec.get(\"category\", \"other\"),\n",
                "    })\n",
                "\n",
                "    # Bonus sample: \"Summarize this about Maruf\" ‚Üí summary\n",
                "    if rec.get(\"summary\"):\n",
                "        instruction_samples.append({\n",
                "            \"instruction\": f\"Summarize Maruf's {rec.get('category', 'background')} information.\",\n",
                "            \"input\":       \"\",\n",
                "            \"output\":      rec[\"summary\"],\n",
                "            \"source\":      rec.get(\"source\", \"unknown\"),\n",
                "            \"category\":    rec.get(\"category\", \"other\"),\n",
                "        })\n",
                "\n",
                "    # Bonus sample: key facts as bullet list\n",
                "    facts = rec.get(\"key_facts\", [])\n",
                "    if facts:\n",
                "        instruction_samples.append({\n",
                "            \"instruction\": f\"List key facts about Maruf's {rec.get('category', 'background')}.\",\n",
                "            \"input\":       \"\",\n",
                "            \"output\":      \"\\n\".join(f\"- {f}\" for f in facts),\n",
                "            \"source\":      rec.get(\"source\", \"unknown\"),\n",
                "            \"category\":    rec.get(\"category\", \"other\"),\n",
                "        })\n",
                "\n",
                "print(f\"‚úÖ Total instruction-tuning samples: {len(instruction_samples)}\")\n",
                "\n",
                "# Preview the first sample\n",
                "if instruction_samples:\n",
                "    print(\"\\n--- Sample Preview ---\")\n",
                "    print(json.dumps(instruction_samples[0], indent=2, ensure_ascii=False))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000014",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 5 ‚Äî Save Locally as `.jsonl`\n",
                "\n",
                "We write every instruction-tuning sample as one JSON object per line.\n",
                "This file can be directly used with most fine-tuning frameworks\n",
                "(Hugging Face Trainer, Axolotl, LLaMA-Factory, etc.)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "a0000015",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Saved 147 samples to:\n",
                        "   G:\\Github_Projects\\Ai_twin\\dataset\\ai_twin_instruction_data.jsonl\n",
                        "   File size: 68.0 KB\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 5: Save to JSONL locally\n",
                "# ============================================================\n",
                "\n",
                "OUTPUT_DIR  = Path(r\"G:\\Github_Projects\\Ai_twin\\dataset\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "JSONL_PATH  = OUTPUT_DIR / \"ai_twin_instruction_data.jsonl\"\n",
                "\n",
                "with open(JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
                "    for sample in instruction_samples:\n",
                "        f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n",
                "\n",
                "print(f\"üíæ Saved {len(instruction_samples)} samples to:\")\n",
                "print(f\"   {JSONL_PATH}\")\n",
                "print(f\"   File size: {JSONL_PATH.stat().st_size / 1024:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000016",
            "metadata": {},
            "source": [
                "### 5b ‚Äî (Optional) Save extracted records for debugging\n",
                "\n",
                "Save the raw structured extraction results as a separate JSON file\n",
                "so you can inspect them later without re-running the API calls."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "a0000017",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Raw extractions saved to: G:\\Github_Projects\\Ai_twin\\dataset\\ai_twin_raw_extractions.json\n"
                    ]
                }
            ],
            "source": [
                "# Optional: save raw extraction results\n",
                "RAW_JSON_PATH = OUTPUT_DIR / \"ai_twin_raw_extractions.json\"\n",
                "\n",
                "with open(RAW_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(extracted_records, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"üíæ Raw extractions saved to: {RAW_JSON_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000018",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 6 ‚Äî Push Dataset to Hugging Face Hub\n",
                "\n",
                "We load the JSONL file into a Hugging Face `Dataset` object and push it to\n",
                "a **private** repository on the Hub.\n",
                "\n",
                "| Setting | Value |\n",
                "|---------|-------|\n",
                "| Username | `marufmullah50` |\n",
                "| Dataset name | `Ai-Twin-data` |\n",
                "| Visibility | **private** |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "a0000019",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "69b8725d7bae42c1a9b6f9652b772395",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split: 0 examples [00:00, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì¶ Dataset loaded: 147 rows\n",
                        "   Columns: ['instruction', 'input', 'output', 'source', 'category']\n",
                        "\n",
                        "   Preview:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'instruction': [\"What are Md. Maruf Mullah's professional background and research interests?\",\n",
                            "  \"Summarize Maruf's biography information.\",\n",
                            "  \"List key facts about Maruf's biography.\"],\n",
                            " 'input': ['', '', ''],\n",
                            " 'output': ['I am a Mechanical Engineer and Researcher with a strong focus on bridging classical engineering and computational intelligence. My research interests include machine learning, materials science, robotics, and renewable energy applications. I have industrial experience at IFAD Autos PLC and PRAN-RFL Group, and I achieved 99.9% accuracy in a casting defect classification project.',\n",
                            "  'Md. Maruf Mullah is a Mechanical Engineer and Researcher focused on integrating classical engineering with computational intelligence to address challenges in materials science, manufacturing, and robotics. He has a B.Sc. in Mechanical Engineering and is actively involved in various research areas including machine learning and smart manufacturing.',\n",
                            "  '- Md. Maruf Mullah is a Mechanical Engineer and Researcher.\\n- He focuses on bridging classical engineering and computational intelligence.\\n- He works on data-driven modeling, machine learning, and computational methods.\\n- He has a B.Sc. in Mechanical Engineering from the Military Institute of Science and Technology (MIST), Dhaka, Bangladesh.\\n- His research interests include machine learning, materials science, robotics, and renewable energy applications.\\n- He has industrial experience at IFAD Autos PLC and PRAN-RFL Group.\\n- He achieved 99.9% accuracy in a casting defect classification project.'],\n",
                            " 'source': ['Personal_Knowledge_Base',\n",
                            "  'Personal_Knowledge_Base',\n",
                            "  'Personal_Knowledge_Base'],\n",
                            " 'category': ['biography', 'biography', 'biography']}"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# ============================================================\n",
                "# Step 6: Push to Hugging Face Hub\n",
                "# ============================================================\n",
                "\n",
                "HF_USERNAME    = \"marufmullah50\"\n",
                "HF_DATASET     = \"Ai-Twin-data\"\n",
                "HF_REPO_ID     = f\"{HF_USERNAME}/{HF_DATASET}\"\n",
                "\n",
                "# Load JSONL into a Hugging Face Dataset\n",
                "dataset = Dataset.from_json(str(JSONL_PATH))\n",
                "\n",
                "print(f\"üì¶ Dataset loaded: {len(dataset)} rows\")\n",
                "print(f\"   Columns: {dataset.column_names}\")\n",
                "print(f\"\\n   Preview:\")\n",
                "dataset[:3]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "a0000020",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "433b1ec83aa04139898cd975c9bfd844",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b87881f5ed824aa8ada78511b93cec5f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "222e4f9442ba4ce7b259973e7ca256dc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1eb8326ca17649c29243fee2f4a6853a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "New Data Upload: |          |  0.00B /  0.00B            "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üöÄ Dataset pushed to: https://huggingface.co/datasets/marufmullah50/Ai-Twin-data\n",
                        "   Visibility: PRIVATE ‚úÖ\n"
                    ]
                }
            ],
            "source": [
                "# Push to Hub (private)\n",
                "dataset.push_to_hub(\n",
                "    repo_id=HF_REPO_ID,\n",
                "    private=True,\n",
                "    token=HF_TOKEN,\n",
                ")\n",
                "\n",
                "print(f\"\\nüöÄ Dataset pushed to: https://huggingface.co/datasets/{HF_REPO_ID}\")\n",
                "print(\"   Visibility: PRIVATE ‚úÖ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a0000021",
            "metadata": {},
            "source": [
                "---\n",
                "## ‚úÖ Done!\n",
                "\n",
                "### What was created\n",
                "\n",
                "| Artifact | Location |\n",
                "|----------|----------|\n",
                "| Instruction-tuning data | `dataset/ai_twin_instruction_data.jsonl` |\n",
                "| Raw extractions backup | `dataset/ai_twin_raw_extractions.json` |\n",
                "| HF Dataset (private) | `huggingface.co/datasets/marufmullah50/Ai-Twin-data` |\n",
                "\n",
                "### Next Steps\n",
                "1. **Review** the JSONL file ‚Äî inspect a few samples for quality\n",
                "2. **Fine-tune** an LLM (e.g., Llama 3, Mistral, Phi-3) using this dataset\n",
                "3. **Iterate** ‚Äî add more data sources and re-run this pipeline\n",
                "4. **Build a RAG system** with the knowledge base for real-time Q&A"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "a0000022",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Final Dataset Statistics\n",
                        "========================================\n",
                        "Total samples:     147\n",
                        "Unique sources:    5\n",
                        "Unique categories: 7\n",
                        "\n",
                        "By Source:\n",
                        "   ChatLogs                            ‚Üí   66 samples\n",
                        "   CV                                  ‚Üí   27 samples\n",
                        "   Personal_Knowledge_Base             ‚Üí   18 samples\n",
                        "   SOP                                 ‚Üí   18 samples\n",
                        "   PersonalStatement                   ‚Üí   18 samples\n",
                        "\n",
                        "By Category:\n",
                        "   other                               ‚Üí   51 samples\n",
                        "   biography                           ‚Üí   33 samples\n",
                        "   education                           ‚Üí   27 samples\n",
                        "   values                              ‚Üí    9 samples\n",
                        "   experience                          ‚Üí    9 samples\n",
                        "   research                            ‚Üí    9 samples\n",
                        "   goals                               ‚Üí    9 samples\n"
                    ]
                }
            ],
            "source": [
                "# Quick dataset stats\n",
                "print(\"üìä Final Dataset Statistics\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Total samples:     {len(instruction_samples)}\")\n",
                "print(f\"Unique sources:    {len(set(s['source'] for s in instruction_samples))}\")\n",
                "print(f\"Unique categories: {len(set(s['category'] for s in instruction_samples))}\")\n",
                "print()\n",
                "\n",
                "# Breakdown by source\n",
                "from collections import Counter\n",
                "source_counts = Counter(s[\"source\"] for s in instruction_samples)\n",
                "print(\"By Source:\")\n",
                "for src, cnt in source_counts.most_common():\n",
                "    print(f\"   {src:35s} ‚Üí {cnt:>4} samples\")\n",
                "\n",
                "print()\n",
                "cat_counts = Counter(s[\"category\"] for s in instruction_samples)\n",
                "print(\"By Category:\")\n",
                "for cat, cnt in cat_counts.most_common():\n",
                "    print(f\"   {cat:35s} ‚Üí {cnt:>4} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f02477a",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llms",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
