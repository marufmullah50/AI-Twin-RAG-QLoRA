{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e93e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbc5e5",
   "metadata": {},
   "source": [
    "## Website Data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "070a52b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped: https://marufmullah50.github.io/\n",
      "Scraped: https://marufmullah50.github.io/index.html\n",
      "Scraped: https://marufmullah50.github.io/about.html\n",
      "Scraped: https://marufmullah50.github.io/education.html\n",
      "Scraped: https://marufmullah50.github.io/skills.html\n",
      "Scraped: https://marufmullah50.github.io/research.html\n",
      "Scraped: https://marufmullah50.github.io/projects.html\n",
      "Scraped: https://marufmullah50.github.io/experience.html\n",
      "Scraped: https://marufmullah50.github.io/contact.html\n",
      "Finished: PersonalWebsite\n",
      "Finished: LinkedIn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------------------\n",
    "# Load env\n",
    "# ------------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------------\n",
    "# Config\n",
    "# ------------------------\n",
    "OUTPUT_FOLDER = \"G:\\\\Github_Projects\\\\Ai_twin\\\\file\\\\CV_statement_details\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "URLS = {\n",
    "    \"PersonalWebsite\": \"https://marufmullah50.github.io/\",\n",
    "    \"LinkedIn\": \"https://www.linkedin.com/in/marufmullah50\"\n",
    "}\n",
    "\n",
    "MAX_PAGES = 30\n",
    "MIN_TEXT_LENGTH = 200   # skip tiny junk pages\n",
    "\n",
    "# ------------------------\n",
    "# Text Cleaner\n",
    "# ------------------------\n",
    "def clean_text(text):\n",
    "    lines = [l.strip() for l in text.splitlines()]\n",
    "    lines = [l for l in lines if l]\n",
    "\n",
    "    # remove obvious navigation/footer noise\n",
    "    blacklist = [\"menu\", \"navigation\", \"login\", \"sign in\", \"cookie\", \"privacy\"]\n",
    "    filtered = []\n",
    "\n",
    "    for line in lines:\n",
    "        low = line.lower()\n",
    "        if not any(b in low for b in blacklist):\n",
    "            filtered.append(line)\n",
    "\n",
    "    return \"\\n\".join(filtered)\n",
    "\n",
    "# ------------------------\n",
    "# Crawl whole site\n",
    "# ------------------------\n",
    "def scrape_site(base_url, max_pages=20):\n",
    "    visited = set()\n",
    "    to_visit = [base_url]\n",
    "    domain = urlparse(base_url).netloc\n",
    "    collected = []\n",
    "\n",
    "    while to_visit and len(visited) < max_pages:\n",
    "        url = to_visit.pop(0)\n",
    "\n",
    "        if url in visited:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # remove script/style\n",
    "            for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "                tag.decompose()\n",
    "\n",
    "            raw_text = soup.get_text(\"\\n\")\n",
    "            cleaned = clean_text(raw_text)\n",
    "\n",
    "            if len(cleaned) > MIN_TEXT_LENGTH:\n",
    "                collected.append(cleaned)\n",
    "                print(f\"Scraped: {url}\")\n",
    "\n",
    "            visited.add(url)\n",
    "\n",
    "            # find internal links\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                full = urljoin(base_url, a[\"href\"]).split(\"#\")[0]\n",
    "                parsed = urlparse(full)\n",
    "\n",
    "                if parsed.netloc == domain and full not in visited:\n",
    "                    to_visit.append(full)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {url} -> {e}\")\n",
    "\n",
    "    # deduplicate pages\n",
    "    unique_pages = list(dict.fromkeys(collected))\n",
    "    return \"\\n\\n\".join(unique_pages)\n",
    "\n",
    "# ------------------------\n",
    "# Single page scrape (fallback)\n",
    "# ------------------------\n",
    "def scrape_single(url):\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    return clean_text(soup.get_text(\"\\n\"))\n",
    "\n",
    "# ------------------------\n",
    "# Run scraping\n",
    "# ------------------------\n",
    "scraped_data = {}\n",
    "\n",
    "for label, url in URLS.items():\n",
    "    try:\n",
    "        if \"github.io\" in url:\n",
    "            scraped_data[label] = scrape_site(url, MAX_PAGES)\n",
    "        else:\n",
    "            scraped_data[label] = scrape_single(url)\n",
    "\n",
    "        print(f\"Finished: {label}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed scraping {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e1f2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#requests.get(\"http://localhost:11434\").content\n",
    "#OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "from openai import OpenAI\n",
    "#ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "#MODEL_o = \"llama3.2\"\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = \"gpt-5-nano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82eced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, label):\n",
    "    system_prompt = (\n",
    "        \"You are an expert AI assistant helping build a structured knowledge base \"\n",
    "        \"about a person from their website content. Your task is to extract factual, \"\n",
    "        \"biographical, academic, professional, and personality-related information \"\n",
    "        \"so it can be used for an AI twin and RAG system.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Extract and organize **all relevant personal and professional information** from the following content taken from {label}.\n",
    "\n",
    "Format the output in **Markdown** with clear sections:\n",
    "\n",
    "## Biography / Background  \n",
    "## Education  \n",
    "## Research Interests  \n",
    "## Technical Skills  \n",
    "## Projects  \n",
    "## Publications / Work Experience  \n",
    "## Achievements  \n",
    "## Goals / Future Plans  \n",
    "## Personal Values or Motivations  \n",
    "## Writing Style or Tone Clues  \n",
    "## Contact / Links (if present)  \n",
    "## Other Important Notes  \n",
    "\n",
    "Rules:\n",
    "- Keep only factual information found in the text  \n",
    "- Do NOT invent anything  \n",
    "- Prefer concise bullet points  \n",
    "- Preserve first-person meaning if written that way  \n",
    "- Ignore menus, navigation, and boilerplate website text  \n",
    "\n",
    "CONTENT:\n",
    "{text[:4000]}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b43b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PersonalWebsite Info (Markdown) ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Biography / Background\n",
       "- Md. Maruf Mullah is a Mechanical Engineer and Researcher with a focus on bridging classical engineering and computational intelligence to solve problems in materials science, manufacturing, and robotics.\n",
       "- I am a dedicated Mechanical Engineer and researcher with a strong focus on bridging the gap between classical engineering and computational intelligence.\n",
       "- I aim to connect traditional engineering fundamentals with cutting-edge computational approaches.\n",
       "- I work on data-driven modeling, machine learning/deep learning, and computational methods to predict material behavior and support engineering decisions.\n",
       "- My work spans from developing YOLO-based computer vision systems for industrial safety and medical imaging to researching the densification of tropical wood species.\n",
       "- I am open to collaboration, research discussions, and innovation-oriented projects.\n",
       "\n",
       "## Education\n",
       "- B.Sc. in Mechanical Engineering, Military Institute of Science and Technology (MIST), Dhaka, Bangladesh\n",
       "  - Apr 2021 â€“ May 2025\n",
       "  - CGPA: 3.23 / 4.00\n",
       "- Higher Secondary Certificate (HSC), Abdul Kadir Mollah City College\n",
       "  - 2020\n",
       "  - GPA: 5.00 / 5.00\n",
       "- Secondary School Certificate (SSC), Siraj Nagar M.A. Pilot High School\n",
       "  - 2018\n",
       "  - GPA: 5.00 / 5.00\n",
       "\n",
       "## Research Interests\n",
       "- Machine Learning & Deep Learning in Mechanical Engineering\n",
       "- Materials Science, Metamaterials\n",
       "- Autonomous Systems & Robotics\n",
       "- Smart Manufacturing\n",
       "- Renewable Energy Applications with Data-Driven Methods\n",
       "\n",
       "## Technical Skills\n",
       "- Programming / Tools: Python, MATLAB\n",
       "- CAD / Design: SOLIDWORKS; CAD/CAM design\n",
       "- Simulation / Analysis: ANSYS; Finite Element Analysis (FEA) awareness\n",
       "- Additive Manufacturing / Prototyping: FDM 3D printing workflows\n",
       "- Other: Experience in surface roughness prediction, autonomous/robotic system development; experience with image-based classification (e.g., YOLO-based computer vision)\n",
       "\n",
       "## Projects\n",
       "- Casting Defect Classification (reported 99.9% accuracy)\n",
       "- Tailstock Tool Holder Design\n",
       "- Surface roughness prediction using machine learning (mentioned in About Me)\n",
       "- Freshwater fish image classification with MobileNetV2 (mentioned in About Me)\n",
       "- Wind forecasting using ANN/CatBoost (mentioned in About Me)\n",
       "- Machining tool design and fabrication (mentioned in About Me)\n",
       "\n",
       "## Publications / Work Experience\n",
       "- Publications / Thesis\n",
       "  - Thesis: Densification of Natural Wood\n",
       "  - Paper: Impact Strength of Seasoned Wood (ICMEAS 2025)\n",
       "- Work Experience\n",
       "  - Industrial experience at IFAD Autos PLC\n",
       "  - Industrial experience at PRAN-RFL Group\n",
       "- Leadership / Other Roles\n",
       "  - Science Fair Champion\n",
       "  - Scout Volunteer\n",
       "\n",
       "## Achievements\n",
       "- Science Fair Champion\n",
       "- Casting Defect Classification project achieving 99.9% accuracy\n",
       "- Leadership as a Scout Volunteer\n",
       "\n",
       "## Goals / Future Plans\n",
       "- Connect materials science, manufacturing methods, and intelligent computational tools to create efficient and practical engineering solutions.\n",
       "- Engage in collaboration, research discussions, and innovation-oriented projects.\n",
       "\n",
       "## Personal Values or Motivations\n",
       "- Passionate about solving complex engineering problems using data-driven approaches.\n",
       "- Committed to bridging classical engineering with computational intelligence.\n",
       "\n",
       "## Writing Style or Tone Clues\n",
       "- Self-descriptive, outcome-focused, and enthusiastic about interdisciplinary integration.\n",
       "- Emphasizes collaboration, data-driven methods, and practical engineering solutions.\n",
       "- Uses action-oriented language (e.g., developing systems, leading projects, applying ML/DL).\n",
       "\n",
       "## Contact / Links (if present)\n",
       "- The content references navigation items like \"Get In Touch\" and \"Contact,\" but no explicit contact details (email, phone, or social links) are provided in the text provided.\n",
       "\n",
       "## Other Important Notes\n",
       "- The content includes two overlapping sections titled \"About Me,\" revealing similar information in multiple places.\n",
       "- Notable technical focus areas mentioned: YOLO-based computer vision for industrial safety/medical imaging, wood densification research, and a mix of traditional mechanical engineering with ML/DL approaches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LinkedIn Info (Markdown) ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Biography / Background\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Education\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Research Interests\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Technical Skills\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Projects\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Publications / Work Experience\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Achievements\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Goals / Future Plans\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Personal Values or Motivations\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Writing Style or Tone Clues\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Contact / Links (if present)\n",
       "- No content provided in the supplied content.\n",
       "\n",
       "## Other Important Notes\n",
       "- Content block is empty; cannot extract information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "cleaned_data = {}\n",
    "for label, text in scraped_data.items():\n",
    "    summary = summarize_text(text, label)\n",
    "    cleaned_data[label] = summary\n",
    "    print(f\"\\n--- {label} Info (Markdown) ---\\n\")\n",
    "    display(Markdown(summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "facb547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal knowledge file saved to: G:\\Github_Projects\\Ai_twin\\file\\CV_statement_details\\Personal_Knowledge_Base.md\n"
     ]
    }
   ],
   "source": [
    "# Save Personal Knowledge Base as MARKDOWN\n",
    "import os\n",
    "\n",
    "output_folder = \"G:\\\\Github_Projects\\\\Ai_twin\\\\file\\\\CV_statement_details\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "md_path = os.path.join(output_folder, \"Personal_Knowledge_Base.md\")\n",
    "\n",
    "with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Personal Knowledge Base\\n\")\n",
    "    f.write(\"Structured personal, academic, and professional information extracted for AI Twin RAG system.\\n\\n\")\n",
    "\n",
    "    for label, text in cleaned_data.items():\n",
    "        # Heading per source file\n",
    "        f.write(f\"## Source: {label}\\n\\n\")\n",
    "        f.write(text.strip())\n",
    "        f.write(\"\\n\\n---\\n\\n\")  # separator\n",
    "\n",
    "print(f\"Personal knowledge file saved to: {md_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956b16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
